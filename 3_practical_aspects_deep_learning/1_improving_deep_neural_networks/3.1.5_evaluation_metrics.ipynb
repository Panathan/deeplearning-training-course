{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>div.item {\n",
       "    vertical-align: bottom;\n",
       "    text-align: center;\n",
       "}\n",
       "img {\n",
       "    background-color: white;\n",
       "}\n",
       ".caption {\n",
       "    display: block;\n",
       "}\n",
       "\n",
       "/* Three image containers (use 25% for four, and 50% for two, etc) */\n",
       ".column {\n",
       "  float: left;\n",
       "  width: 50%;\n",
       "  padding: 5px;\n",
       "}\n",
       "\n",
       "/* Clear floats after image containers */\n",
       ".row::after {\n",
       "  content: \"\";\n",
       "  clear: both;\n",
       "  display: table;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import utils\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "utils.set_css_style('style.css')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Evaluation Metrics\n",
    "\n",
    "Choosing the right metric is crucial while evaluating machine learning (ML) models. Various metrics are proposed to evaluate ML models in different applications. In some applications looking at a single metric may not give you the whole picture of the problem you are solving, and you may want to use a subset of the metrics discussed in this post to have a concrete evaluation of your models.\n",
    "\n",
    "Whether you're tuning hyperparameters, or trying out different ideas for learning algorithms, or just trying out different options for building your machine learning system. You'll find that your progress will be much faster if you have a single real number evaluation metric that lets you quickly rank ideas and hyperparameters. Therefore, for a successful machine learning project, setting up a single real number evaluation metric is a key. \n",
    "\n",
    "Applied machine learning is a very empirical process. We often have an idea, code it up, run the experiment to see how it did, and then use the outcome of the experiment to refine the ideas. And then keep going around this loop as you keep on improving your algorithm.\n",
    "\n",
    "<img src=\"figures/ml-idea-iteration.png\" alt=\"ml-idea-iteration\" style=\"width: 400px;\"/>\n",
    "\n",
    "Nevertheless, sometimes it's not always easy to combine all the things you care about into a single evaluation metric. In those cases, it is sometimes useful to set up **satisficing matrics** as well as the **optimizing matric**. \n",
    "\n",
    "As a side note, it is also worth mentioning that the evaluation metric is different from loss function. Loss functions are functions that show a measure of the model performance and are used to train a machine learning model (using some kind of optimization), and are usually differentiable in model’s parameters. On the other hand, metrics are used to monitor and measure the performance of a model (during training, and test), and do not need to be differentiable. However if for some tasks the performance metric is differentiable, it can be used both as a loss function (perhaps with some regularizations added to it), and a evaluation metric, such as MSE.\n",
    "\n",
    "Metrics can be grouped into different categories based on the ML model/application they are mostly used for:\n",
    "\n",
    "* Classification Metrics (accuracy, precision, recall, F1-score, ROC, AUC, …)\n",
    "* Regression Metrics (MSE, MAE)\n",
    "* Statistical Metrics (Correlation)\n",
    "* Computer Vision Metrics (PSNR, SSIM, IoU)\n",
    "* NLP Metrics (Perplexity, BLEU score)\n",
    "* Deep Learning Related Metrics (Inception score, Frechet Inception distance)\n",
    "\n",
    "Here we will focus on classification and regression metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Regression related metrics\n",
    "\n",
    "\n",
    "Regression models are another family of machine learning and statistical models, which are used to predict a continuous target values³. They have a wide range of applications, from house price prediction, E-commerce pricing systems, weather forecasting, stock market prediction, to image super resolution, feature learning via auto-encoders, and image compression.\n",
    "\n",
    "Metrics used to evaluate these models should be able to work on a set of continuous values (with infinite cardinality), and are therefore slightly different from classification metrics.\n",
    "\n",
    "\n",
    "## 2.1. MSE\n",
    "\n",
    "“Mean squared error” is perhaps the most popular metric used for regression problems. It essentially finds the average squared error between the predicted and actual values.\n",
    "\n",
    "Let’s assume we have a regression model which predicts the price of houses in Seattle area (denoted by $\\hat{y}_i$ ), and let’s say for each house we also have the actual price the house was sold for (denoted by $y_i$). Then the MSE can be calculated as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathsf{MSE} = \\frac{1}{m} \\sum_i^m (\\hat{y}_i - y_i)^2\n",
    "\\end{equation}\n",
    "\n",
    "Where $m$ is the number of observations. The use of square distance allow us to penalize the large distances extremely.\n",
    "\n",
    "<img src=\"figures/mse.jpg\" alt=\"mse\" style=\"width: 500px;\"/>\n",
    "\n",
    "## 2.2. RMSE \n",
    "\n",
    "**RMSE** is just the square root of MSE. The square root is introduced to make scale of the errors to be the same as the scale of targets.\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathsf{RMSE} = \\sqrt{\\frac{1}{m} \\sum_i^m (\\hat{y}_i - y_i)^2}\n",
    "\\end{equation}\n",
    "\n",
    "Looking at house pricing prediction, RMSE essentially shows what is the average deviation in your model predicted house prices in dollars (same unit) from the target values (the prices the houses are sold for).\n",
    "\n",
    "\n",
    "## 2.3. MAE\n",
    "\n",
    "Mean absolute error (or mean absolute deviation) is another metric which finds the average absolute distance between the predicted and target values. MAE is define as below:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathsf{MAE} = \\frac{1}{m} \\sum_i^m |\\hat{y}_i - y_i|\n",
    "\\end{equation}\n",
    "\n",
    "MAE is known to be more robust to the outliers than MSE. The main reason being that in MSE by squaring the errors, the outliers (which usually have higher errors than other samples) get more attention and dominance in the final error and impacting the model parameters.\n",
    "\n",
    "## 2.4. R-Squared\n",
    "\n",
    "R Squared is a measurement that tells you to what extent the proportion of variance in the dependent variable (target) is explained by the variance in the predictor variables. In simpler terms, while the coefficients estimate trends, R-squared represents the scatter around the line of best fit.\n",
    "\n",
    "For example, if the R² is 0.80, then 80% of the variation can be explained by the model’s inputs.\n",
    "If the R² is 1.0 or 100%, that means that all movements of the dependent variable can be entirely explained by the movements of the independent variables.\n",
    "To show a visual example, despite having the same line of best fit, the R² on the right is much higher than the one on the left.\n",
    "\n",
    "<img src=\"figures/r-squared.png\" alt=\"r-squared\" style=\"width: 800px;\"/>\n",
    "\n",
    "The equation for R² is as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathsf{R^2} = 1 - \\frac{\\mathsf{Explained\\ Variation}}{\\mathsf{Total\\ Variation}}\n",
    "\\end{equation}\n",
    "\n",
    "The Explained Variation is equal to the sum of squared residuals while the total variation is equal to the total sum of squared.\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathsf{SS_{residual}} = \\sum_{i=0}^{m} (y_i  -  \\hat{y}_i)^2  \\\\\n",
    "\\mathsf{SS_{total}} = \\sum_{i=0}^{m} (y_i  -  \\bar{y}_i)^2\n",
    "\\end{align*}\n",
    "\n",
    "## 2.5. Adjusted R-Squared\n",
    "\n",
    "Every additional independent variable added to a model always increases the R² value — therefore, a model with several independent variables may seem to be a better fit even if it isn’t. This is where Adjusted R² comes in. The adjusted R² compensates for each additional independent variable and only increases if the new term improves the model more than would be expected by chance.\n",
    "\n",
    "\n",
    "The formula for the Adjusted R-Squared taking into consideration the number of predictors $p$ of the model is then:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathsf{R^2_{adjusted}} = 1 - \\frac{(1 - \\mathsf{R^2})(m-1)}{m-p-1}\n",
    "\\end{equation}\n",
    "\n",
    "While values are usually positive, they can be negative as well. This could happen if your $R^2$ is zero; After the adjustment, the value can dip below zero. This usually indicates that your model is a poor fit for your data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Classification related metrics\n",
    "\n",
    "Classification is one of the most widely used problems in machine learning with various industrial applications, from face recognition, Youtube video categorization, content moderation, medical diagnosis, to text classification, hate speech detection on Twitter.\n",
    "\n",
    "There are various ways to evaluate a classification model, in this section we'll be covering some of the most popular ones below.\n",
    "\n",
    "## 3.1. Confusion Matrix (not a metric, but important to know!)\n",
    "\n",
    "One of the key concept in classification performance is the confusion matrix, which is a tabular visualization of the model predictions versus the ground-truth labels. Each row of confusion matrix represents the instances in a predicted class and each column represents the instances in an actual class.\n",
    "\n",
    "Let’s go through this with an example. Let’s assume we are building a binary classification to classify cat images from non-cat images. And let’s assume our test set has 1100 images (1000 non-cat images, and 100 cat images), with the below confusion matrix.\n",
    "\n",
    "<img src=\"figures/cats-cm.png\" alt=\"cats-cm\" style=\"width: 500px;\"/>\n",
    "\n",
    "* Out of 100 cat images the model has predicted 90 of them correctly and has mis-classified 10 of them. If we refer to the “cat” class as positive and the non-cat class as negative class, then 90 samples predicted as cat are considered as as true positive (TP), and the 10 samples predicted as non-cat are false negative (FN).\n",
    "\n",
    "* Out of 1000 non-cat images, the model has classified 940 of them correctly, and mis-classified 60 of them. The 940 correctly classified samples are referred as true negative (TN), and those 60 are referred as false positive (FP).\n",
    "\n",
    "As we can see diagonal elements of this matrix denote the correct prediction for different classes, while the off-diagonal elements denote the samples which are mis-classified.\n",
    "\n",
    "Below is a more general representation of the confusion matrix:\n",
    "\n",
    "<img src=\"figures/confusion-matrix.png\" alt=\"confusion-matrix\" style=\"width: 500px;\"/>\n",
    "\n",
    "Now that we have a better understanding of the confusion matrix, let’s get into the actual metrics.\n",
    "\n",
    "## 3.2. Classification Accuracy\n",
    "\n",
    "Classification accuracy is perhaps the simplest metrics one can imagine, and is defined as the number of correct predictions divided by the total number of predictions. \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathsf{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN} \n",
    "\\end{equation}\n",
    "\n",
    "So in the above example, out of 1100 samples 1030 are predicted correctly, resulting in a classification accuracy of:\n",
    "\n",
    "**Classification accuracy**= (90+940)/(1000+100)= 1030/1100= 93.6%\n",
    "\n",
    "\n",
    "## 3.3. Precision\n",
    "\n",
    "There are many cases in which classification accuracy is not a good indicator of your model performance. One of these scenarios is when your class distribution is imbalanced (one class is more frequent than others). In this case, even if you predict all samples as the most frequent class you would get a high accuracy rate, which does not make sense at all (because your model is not learning anything, and is just predicting everything as the top class). \n",
    "\n",
    "For example in our cat vs non-cat classification above, if the model predicts all samples as non-cat, it would result in a 1000/1100= 90.9%.\n",
    "\n",
    "Therefore we need to look at class specific performance metrics too. Precision is one of such metrics, which is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathsf{Precision} = \\frac{TP}{TP + FP} \n",
    "\\end{equation}\n",
    "\n",
    "**The precision of the model is interpreted as follows: out of those positively predicted, what is the percentage of observations that are actually positive**.\n",
    "\n",
    "The precision of Cat and Non-Cat class in the above example can be calculated as:\n",
    "\n",
    "**Precision_Cat** = #samples correctly predicted cat/#samples predicted as cat = 90 / (90 + 60) = 60%\n",
    "\n",
    "**Precision_NonCat** = 940 / 950 = 98.9%\n",
    "\n",
    "As we can see the model has much higher precision in predicting non-cat samples, versus cats. This is not surprising, as model has seen more examples of non-cat images during training, making it better in classifying that class.\n",
    "\n",
    "\n",
    "## 3.4. Recall \n",
    "\n",
    "Recall is another important metric, which is defined as the fraction of samples from a class which are correctly predicted by the model. More formally:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathsf{Recall} = \\frac{TP}{TP + FN} \n",
    "\\end{equation}\n",
    "\n",
    "Therefore, for our example above, the recall rate of cat and non-cat classes can be found as:\n",
    "\n",
    "**Recall_Cat** = 90 / 100 = 90%\n",
    "\n",
    "**Recall_NonCat** = 940 / 1000 = 94%\n",
    "\n",
    "## 3.5. F1 Score\n",
    "\n",
    "Depending on application, you may want to give higher priority to recall or precision. But there are many applications in which both recall and precision are important. Therefore, it is natural to think of a way to combine these two into a single metric. One popular metric which combines precision and recall is called F1-score, which is the harmonic mean of precision and recall defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathsf{F1score} = 2*\\frac{\\mathsf{Precision} \\cdot \\mathsf{Recall}}{\\mathsf{Precision}+\\mathsf{Recall}} \n",
    "\\end{equation}\n",
    "\n",
    "### Why is the F-Measure a harmonic mean and not an arithmetic mean of the Precision and Recall measures?\n",
    "\n",
    "This is just for a quick reference to understand the nature of the arithmetic mean and the harmonic mean with plots. As you can see from the plot, consider the X axis and Y axis as precision and recall, and the Z axis as the F1 Score. So, from the plot of the harmonic mean, both the precision and recall should contribute evenly for the F1 score to rise up unlike the Arithmetic mean.\n",
    "\n",
    "This is for the arithmetic mean.\n",
    "\n",
    "<img src=\"figures/arithmetic-mean.jpg\" alt=\"arithmetic-mean\" style=\"width: 300px;\"/>\n",
    "\n",
    "This is for the harmonic mean.\n",
    "\n",
    "<img src=\"figures/harmonic-mean.jpg\" alt=\"harmonic-mean\" style=\"width: 300px;\"/>\n",
    "\n",
    "So for our cat classification example, the F1-score can be calculated as:\n",
    "\n",
    "**F1-score**= 2 * 0.6 * 0.9  / (0.6 + 0.9) = 72%\n",
    "\n",
    "The generalized version of F-score is defined as below. As we can see F1-score is special case of $F_{\\beta }$ when $\\beta= 1$, where $\\beta$ is chosen such that recall is considered $\\beta$ times as important as precision.\n",
    "\n",
    "\\begin{equation}\n",
    "{\\displaystyle F_{\\beta }=(1+\\beta ^{2})\\cdot {\\frac {\\mathsf {Precision} \\cdot \\mathsf {Recall} }{(\\beta ^{2}\\cdot \\mathsf {Precision} )+\\mathsf {Recall} }}}\n",
    "\\end{equation}\n",
    "\n",
    "It is good to mention that there is always a trade-off between precision and recall of a model, if you want to make the precision too high, you would end up seeing a drop in the recall rate, and vice versa.\n",
    "\n",
    "## 3.6. Sensitivity and Specificity\n",
    "\n",
    "Sensitivity and specificity are two other popular metrics mostly used in medical and biology related fields, and are defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "{\\displaystyle \\mathsf{Sensitivity} = \\mathsf{Recall} = \\frac{TP}{TP+FN} }\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathsf{Specificity} = \\mathsf{True \\ Negative \\ Rate} = \\frac{TN}{TN+FP}\n",
    "\\end{equation}\n",
    "\n",
    "## 3.7. Receiver Operating Characteristic Curve\n",
    "\n",
    "The receiver operating characteristic (ROC) curve is plot which shows the performance of a binary classifier as function of its cut-off threshold. It essentially shows the true positive rate (TPR) against the false positive rate (FPR) for various threshold values. Let’s explain more.\n",
    "Many of the classification models are probabilistic, i.e. they predict the probability of a sample being a cat. They then compare that output probability with some cut-off threshold and if it is larger than the threshold they predict its label as cat, otherwise as non-cat. As an example your model may predict the below probabilities for 4 sample images: [0.45, 0.6, 0.7, 0.3]. Then depending on the threshold values below, you will get different labels:\n",
    "\n",
    "* cut-off= 0.5: predicted-labels= [0,1,1,0] (default threshold)\n",
    "* cut-off= 0.2: predicted-labels= [1,1,1,1]\n",
    "* cut-off= 0.8: predicted-labels= [0,0,0,0]\n",
    "\n",
    "As you can see by varying the threshold values, we will get completely different labels. And as you can imagine each of these scenarios would result in a different precision and recall (as well as TPR, FPR) rates.\n",
    "\n",
    "ROC curve essentially finds out the TPR and FPR for various threshold values and plots TPR against the FPR. A sample ROC curve is shown in Figure below.\n",
    "\n",
    "<img src=\"figures/roc-curve.png\" alt=\"roc-curve\" style=\"width: 500px;\"/>\n",
    "\n",
    "As we can see from this example, the lower the cut-off threshold on positive class, the more samples predicted as positive class, i.e. higher true positive rate (recall) and also higher false positive rate (corresponding to the right side of this curve). Therefore, there is a trade-off between how high the recall could be versus how much we want to bound the error (FPR).\n",
    "ROC curve is a popular curve to look at overall model performance and pick a good cut-off threshold for the model.\n",
    "\n",
    "## 3.8. AUC\n",
    "\n",
    "The area under the curve (AUC), is an aggregated measure of performance of a binary classifier on all possible threshold values (and therefore it is threshold invariant).\n",
    "\n",
    "AUC calculates the area under the ROC curve, and therefore it is between 0 and 1. **One way of interpreting AUC is as the probability that the model ranks a random positive example more highly than a random negative example**.\n",
    "\n",
    "<img src=\"figures/AUC.png\" alt=\"AUC\" style=\"width: 400px;\"/>\n",
    "\n",
    "On high-level, the higher the AUC of a model the better it is. But sometimes threshold independent measure is not what you want, e.g. you may care about your model recall and require that to be higher than 99% (while it has a reasonable precision or FPR). In that case, you may want to tune your model threshold such that it meets your minimum requirement on those metrics (and you may not care if you model AUC is not too high).\n",
    "\n",
    "Therefore in order to decide how to evaluate your classification model performance, perhaps you want to have a good understanding of the business/problem requirement and the impact of low recall vs. low precision, and decide what metric to optimize for.\n",
    "\n",
    "From a practical standpoint, a classification model which outputs probabilities is preferred over a single label output, as it provides the flexibility of tuning the threshold such that it meets your minimum recall/precision requirements. Not all models provide this nice probabilistic outputs though, e.g. SVM does not provide a simple probability as an output (although it provides margin which can be used to tune the decision, but it is not as straightforward and interpretable as having output probabilities)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluation Metrics with Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "m = tf.keras.metrics.RootMeanSquaredError()\n",
    "m.update_state([0, 1, 1, 0], [0, 0, 1, 0])\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "m = tf.keras.metrics.RootMeanSquaredError()\n",
    "m.update_state([0, 1, 1, 0], [0, 0, 1, 0], sample_weight=[0,0,1,0])\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usage with compile() API:\n",
    "\n",
    "```python\n",
    "model.compile(optimizer='sgd',\n",
    "              loss='mse',\n",
    "              metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "m = tf.keras.metrics.MeanSquaredError()\n",
    "m.update_state([0, 1, 1, 0], [0, 0, 1, 0])\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usage with compile() API:\n",
    "\n",
    "```python\n",
    "model.compile(optimizer='sgd',\n",
    "              loss='mse',\n",
    "              metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "m = tf.keras.metrics.MeanAbsoluteError()\n",
    "m.update_state([0, 1, 1, 0], [0, 0, 1, 0])\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usage with compile() API:\n",
    "\n",
    "```python\n",
    "model.compile(optimizer='sgd',\n",
    "              loss='mse',\n",
    "              metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71355796"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = tf.keras.metrics.BinaryCrossentropy()\n",
    "m.update_state([[0, 1], [0, 1]], [[0.6, 0.4], [0.4, 0.6]])\n",
    "m.result().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7135581778200728"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "-(1*np.log(0.4) + 1*np.log(0.6))/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.045855045"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.reset_states()\n",
    "m.update_state([[1], [2], [3], [4]], [[0], [2], [3], [4]],\n",
    "               sample_weight=[1, 1, 0, 0])\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Crossentropy\n",
    "\n",
    "For multi-class classification\n",
    "\n",
    "* If your labels are one-hot encoded: use tf.keras.metrics.CategoricalCrossentropy\n",
    "\n",
    "* If your labels are encoded as integers: use tf.keras.metrics.SparseCategoricalCrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1769392"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = tf.keras.metrics.CategoricalCrossentropy()\n",
    "m.update_state([[0, 1, 0], [0, 0, 1]],\n",
    "               [[0.05, 0.95, 0], [0.1, 0.8, 0.1]])\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1769392"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = tf.keras.metrics.SparseCategoricalCrossentropy()\n",
    "m.update_state([1, 2],\n",
    "               [[0.05, 0.95, 0], [0.1, 0.8, 0.1]])\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = tf.keras.metrics.Accuracy()\n",
    "m.update_state([0, 1, 1, 1, 0], [1, 0, 1, 1, 0])\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666667"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = tf.keras.metrics.Recall()\n",
    "m.update_state([0, 1, 1, 1, 0], [1, 0, 1, 1, 0])\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666667"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = tf.keras.metrics.Precision()\n",
    "m.update_state([0, 1, 1, 1, 0], [1, 0, 1, 1, 0])\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5833334"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = tf.keras.metrics.AUC()\n",
    "m.update_state([0, 1, 1, 1, 0], [1, 0, 1, 1, 0])\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True Positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = tf.keras.metrics.TruePositives()\n",
    "m.update_state([0, 1, 1, 1, 0], [1, 0, 1, 1, 0])\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True Negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = tf.keras.metrics.TrueNegatives()\n",
    "m.update_state([0, 1, 1, 1, 0], [1, 0, 1, 1, 0])\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False Positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = tf.keras.metrics.FalsePositives()\n",
    "m.update_state([0, 1, 1, 1, 0], [1, 0, 1, 1, 0])\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False Negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = tf.keras.metrics.FalseNegatives()\n",
    "m.update_state([0, 1, 1, 1, 0], [0, 1, 0, 0, 0])\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usage with compile() API:\n",
    "\n",
    "```python\n",
    "model.compile(optimizer='sgd',\n",
    "              loss='mse',\n",
    "              metrics=[tf.keras.metrics.FalsePositives()])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Addons\n",
    "\n",
    "With the introduction of TensorFlow 2.0, they've created a new Special Interest Group (SIG) known as TensorFlow Addons. This group governs a repository of contributions that conform to well-established API patterns, but implement new functionality not available in core TensorFlow. As an example these new functionalities may be new algorithms from published papers or missing functionality for data preprocessing and filtering. Check it out on Github: https://github.com/tensorflow/addons.\n",
    "\n",
    "As a community managed SIG, Addons enables users to introduce new extensions to the TensorFlow eco-system in a sustainable manner. The repository follows a modular approach with subpackages and submodules who are maintained by designated community members. As of today, these subpackages include:\n",
    "* tfa.activations\n",
    "* tfa.callbacks\n",
    "* tfa.image\n",
    "* tfa.layers\n",
    "* tfa.losses\n",
    "* tfa.metrics\n",
    "* tfa.optimizers\n",
    "* tfa.rnn\n",
    "* tfa.seq2seq\n",
    "* tfa.text\n",
    "\n",
    "### Moving from tf.contrib\n",
    "\n",
    "The project’s objective may sound quite familiar, and Addons is indeed the landing place for much of tf.contrib which has been moved out of the central TensorFlow repository. Over the past few years, a lot of code in tf.contrib became stale, unmaintained, complicated, and very difficult to use because of documentation. A lot of custom API’s got created in this tf.contrib which made it even harder to integrate this functionality into your code. By moving the most relevant algorithms into TensorFlow-Addons and cleaning them up, much of the code you used in tf.contrib will be available in addons as well. Additionally, a lot of new functionality is already part of the TF-Addons and there is much more planned.\n",
    "\n",
    "### How to Install\n",
    "\n",
    "TensorFlow Addons provides a pip package for macOS and Linux, with plans to support Windows and Anaconda in the future. Try it today on the most recent release of tensorflow-2.0:\n",
    "\n",
    "```python\n",
    "pip install tensorflow-addons\n",
    "```\n",
    "\n",
    "To use TensorFlow-addons in your Python code you can simply import the package with:\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using tf.math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
       "array([[1, 1],\n",
       "       [1, 2]], dtype=int32)>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.math.confusion_matrix([0, 1, 1, 1, 0], [1, 0, 1, 1, 0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using tensorflow_addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[[2. 1.]\n",
      "  [1. 1.]]\n",
      "\n",
      " [[1. 1.]\n",
      "  [1. 2.]]]\n"
     ]
    }
   ],
   "source": [
    "# multilabel confusion matrix\n",
    "y_true = tf.constant([[1, 0], [0, 1], [0, 1], [0, 1], [1, 0]],\n",
    "         dtype=tf.int32)\n",
    "y_pred = tf.constant([[0, 1], [1, 0], [0, 1], [0, 1], [1, 0]],\n",
    "         dtype=tf.int32)\n",
    "output = tfa.metrics.MultiLabelConfusionMatrix(num_classes=2)\n",
    "output.update_state(y_true, y_pred)\n",
    "print('Confusion matrix:')\n",
    "print(output.result().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[[1. 0.]\n",
      "  [0. 1.]]\n",
      "\n",
      " [[1. 0.]\n",
      "  [0. 1.]]\n",
      "\n",
      " [[0. 1.]\n",
      "  [1. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# multilabel confusion matrix\n",
    "y_true = tf.constant([[1, 0, 1], [0, 1, 0]],\n",
    "         dtype=tf.int32)\n",
    "y_pred = tf.constant([[1, 0, 0],[0, 1, 1]],\n",
    "         dtype=tf.int32)\n",
    "output = tfa.metrics.MultiLabelConfusionMatrix(num_classes=3)\n",
    "output.update_state(y_true, y_pred)\n",
    "print('Confusion matrix:')\n",
    "print(output.result().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[[1. 0.]\n",
      "  [0. 1.]]\n",
      "\n",
      " [[1. 0.]\n",
      "  [1. 0.]]\n",
      "\n",
      " [[1. 1.]\n",
      "  [0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# if multiclass input is provided\n",
    "y_true = tf.constant([[1, 0, 0], [0, 1, 0]],\n",
    "         dtype=tf.int32)\n",
    "y_pred = tf.constant([[1, 0, 0],[0, 0, 1]],\n",
    "         dtype=tf.int32)\n",
    "output = tfa.metrics.MultiLabelConfusionMatrix(num_classes=3)\n",
    "output.update_state(y_true, y_pred)\n",
    "print('Confusion matrix:')\n",
    "print(output.result().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlcourse-venv",
   "language": "python",
   "name": "dlcourse-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
