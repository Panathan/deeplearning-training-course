{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>div.item {\n",
       "    vertical-align: bottom;\n",
       "    text-align: center;\n",
       "}\n",
       "img {\n",
       "    background-color: white;\n",
       "}\n",
       ".caption {\n",
       "    display: block;\n",
       "}\n",
       "\n",
       "/* Three image containers (use 25% for four, and 50% for two, etc) */\n",
       ".column {\n",
       "  float: left;\n",
       "  width: 50%;\n",
       "  padding: 5px;\n",
       "}\n",
       "\n",
       "/* Clear floats after image containers */\n",
       ".row::after {\n",
       "  content: \"\";\n",
       "  clear: both;\n",
       "  display: table;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import utils\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "utils.set_css_style('style.css')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Why split the data?\n",
    "\n",
    "To use an analogy, let’s say you teach a child to multiply by letting the kid train on the small multiplication table, i.e. everything from 1x1 to 9x9.\n",
    "\n",
    "Next, you test whether the kid is able to perform the same multiplications. The result is a success. The kid gets it right almost every time.\n",
    "\n",
    "<img src=\"figures/multiplication.jpeg\" alt=\"multiplication\" style=\"width: 300px;\"/>\n",
    "\n",
    "What’s the problem here?\n",
    "\n",
    "You don’t know if the kid understands multiplication at all, or has simply memorized the table!\n",
    "\n",
    "So what you would do instead is test the kid on multiplications like 11x12, that are outside of the table.\n",
    "\n",
    "This is exactly why we need to test machine learning models on unseen data. Otherwise, we have no way of knowing whether the algorithm has learned a generalizable pattern or has simply memorized the training data.\n",
    "\n",
    "# 2. Training, validation & test datasets\n",
    "\n",
    "Data splits usually depends on the use case. But typically, we split data into 3 main datasets:\n",
    "\n",
    "* Training dataset\n",
    "* Validation dataset\n",
    "* Testing dataset\n",
    "\n",
    "<img src=\"figures/splits.png\" alt=\"splits\" style=\"width: 500px;\"/>\n",
    "\n",
    "Let's quote the base definitions from Jason Brownlee’s excellent article on this topic, as it is quite comprehensive:\n",
    "\n",
    "## 2.1. Training dataset\n",
    "\n",
    "> Training Dataset: The sample of data used to fit the model.\n",
    "\n",
    "The actual dataset that we use to train the model (weights and biases in the case of Neural Network). The model sees and learns from this data.\n",
    "\n",
    "\n",
    "## 2.2. Validation dataset\n",
    "\n",
    "> Validation Dataset: The sample of data used to provide an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters. The evaluation becomes more biased as skill on the validation dataset is incorporated into the model configuration.\n",
    "\n",
    "The validation set is used to evaluate a given model, but this is for frequent evaluation. We as machine learning engineers use this data to fine-tune the model hyperparameters. Hence the model occasionally sees this data, but never does it “Learn” from this. We use the validation set results and update higher level hyperparameters. So the validation set in a way affects a model, but indirectly. The validation set could also be called the **developement set**.\n",
    "\n",
    "## 2.3. Test dataset\n",
    "\n",
    "> Test Dataset: The sample of data used to provide an unbiased evaluation of a final model fit on the training dataset.\n",
    "\n",
    "The Test dataset provides the gold standard used to evaluate the model. It is only used once a model is completely trained (using the train and validation sets). The test set is generally what is used to evaluate competing models (For example on many Kaggle competitions, the validation set is released initially along with the training set and the actual test set is only released when the competition is about to close, and it is the result of the the model on the Test set that decides the winner). Many a times the validation set is used as the test set, but it is not good practice. The test set is generally well curated. It contains carefully sampled data that spans the various classes that the model would face, when used in the real world.\n",
    "\n",
    "## 2.4. Dataset split ratio\n",
    "\n",
    "This mainly depends on 2 things. First, the total number of samples in your data and second, on the actual model you are training.\n",
    "\n",
    "Some models need substantial data to train upon, so in this case you would optimize for the larger training sets. Models with very few hyperparameters will be easy to validate and tune, so you can probably reduce the size of your validation set, but if your model has many hyperparameters, you would want to have a large validation set as well(although you should also consider cross validation). Also, if you happen to have a model with no hyperparameters or ones that cannot be easily tuned, you probably don’t need a validation set too!\n",
    "\n",
    "Like many other things in machine learning, the train-test-validation split ratio is also quite specific to your use case and it gets easier to make judge ment as you train and build more and more models.\n",
    "\n",
    "\n",
    "# 3. Cross-validation\n",
    "\n",
    "When there is never enough data to train your model, removing a part of it for validation poses a problem of underfitting. By reducing the training data, we risk losing important patterns/ trends in data set, which in turn increases error induced by bias. So, what we require is a method that provides ample data for training the model and also leaves ample data for validation. Cross validation does exactly that.\n",
    "\n",
    "## 3.1. K-fold cross validation\n",
    "\n",
    "In K Fold cross validation, the data is divided into k subsets. Now the holdout method is repeated k times, such that each time, one of the k subsets is used as the test set/ validation set and the other k-1 subsets are put together to form a training set. The error estimation is averaged over all k trials to get total effectiveness of our model. As can be seen, every data point gets to be in a validation set exactly once, and gets to be in a training set k-1 times. This significantly reduces bias as we are using most of the data for fitting, and also significantly reduces variance as most of the data is also being used in validation set. Interchanging the training and test sets also adds to the effectiveness of this method. As a general rule and empirical evidence, K = 5 or 10 is generally preferred, but nothing’s fixed and it can take any value.\n",
    "\n",
    "<img src=\"figures/cross-validation.png\" alt=\"cross-validation\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    "## 3.2. Stratified k-fold cross validation\n",
    "\n",
    "In some cases, there may be a large imbalance in the response variables. For example, in dataset concerning price of houses, there might be large number of houses having high price. Or in case of classification, there might be several times more negative samples than positive samples. For such problems, a slight variation in the K Fold cross validation technique is made, such that each fold contains approximately the same percentage of samples of each target class as the complete set, or in case of regression problems, the mean response value is approximately equal in all the folds. This variation is also known as Stratified K Fold.\n",
    "\n",
    "These validation techniques are also referred to as Non-exhaustive cross validation methods. These do not compute all ways of splitting the original sample. Exhaustive Methods, on the other hand, compute all possible ways the data can be split into training and test sets.\n",
    "\n",
    "## 3.3. Leave-P-Out cross validation\n",
    "\n",
    "This approach leaves p data points out of training data, i.e. if there are n data points in the original sample then, n-p samples are used to train the model and p points are used as the validation set. This is repeated for all combinations in which original sample can be separated this way, and then the error is averaged for all trials, to give overall effectiveness.\n",
    "\n",
    "This method is exhaustive in the sense that it needs to train and validate the model for all possible combinations, and for moderately large p, it can become computationally infeasible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Splitting data with sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s see how to do split a dataset into a training and a testing datasets in Python. We’ll do this using the `scikit-learn` library and specifically the `train_test_split` method. We’ll start with importing the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s load in the diabetes dataset, turn it into a data frame and define the columns’ names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape:  (442, 10) (442,)\n"
     ]
    }
   ],
   "source": [
    "# Load the Diabetes dataset\n",
    "columns = [\"age\",\"sex\",\"bmi\",\"map\",\"tc\",\"ldl\",\"hdl\",\"tch\",\"ltg\",\"glu\"] # columns names\n",
    "diabetes = datasets.load_diabetes() # Call the diabetes dataset from sklearn\n",
    "df = pd.DataFrame(diabetes.data, columns=columns) # load the dataset as a pandas data frame\n",
    "y = diabetes.target # define the target variable (dependent variable) as y\n",
    "print(\"Dataset shape: \", df.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _diabetes_dataset:\n",
      "\n",
      "Diabetes dataset\n",
      "----------------\n",
      "\n",
      "Ten baseline variables, age, sex, body mass index, average blood\n",
      "pressure, and six blood serum measurements were obtained for each of n =\n",
      "442 diabetes patients, as well as the response of interest, a\n",
      "quantitative measure of disease progression one year after baseline.\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "  :Number of Instances: 442\n",
      "\n",
      "  :Number of Attributes: First 10 columns are numeric predictive values\n",
      "\n",
      "  :Target: Column 11 is a quantitative measure of disease progression one year after baseline\n",
      "\n",
      "  :Attribute Information:\n",
      "      - age     age in years\n",
      "      - sex\n",
      "      - bmi     body mass index\n",
      "      - bp      average blood pressure\n",
      "      - s1      tc, T-Cells (a type of white blood cells)\n",
      "      - s2      ldl, low-density lipoproteins\n",
      "      - s3      hdl, high-density lipoproteins\n",
      "      - s4      tch, thyroid stimulating hormone\n",
      "      - s5      ltg, lamotrigine\n",
      "      - s6      glu, blood sugar level\n",
      "\n",
      "Note: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times `n_samples` (i.e. the sum of squares of each column totals 1).\n",
      "\n",
      "Source URL:\n",
      "https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\n",
      "\n",
      "For more information see:\n",
      "Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\n",
      "(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\n"
     ]
    }
   ],
   "source": [
    "print(diabetes.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize our features table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>map</th>\n",
       "      <th>tc</th>\n",
       "      <th>ldl</th>\n",
       "      <th>hdl</th>\n",
       "      <th>tch</th>\n",
       "      <th>ltg</th>\n",
       "      <th>glu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.038076</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.061696</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>-0.044223</td>\n",
       "      <td>-0.034821</td>\n",
       "      <td>-0.043401</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.019908</td>\n",
       "      <td>-0.017646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.001882</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.051474</td>\n",
       "      <td>-0.026328</td>\n",
       "      <td>-0.008449</td>\n",
       "      <td>-0.019163</td>\n",
       "      <td>0.074412</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.068330</td>\n",
       "      <td>-0.092204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.085299</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.044451</td>\n",
       "      <td>-0.005671</td>\n",
       "      <td>-0.045599</td>\n",
       "      <td>-0.034194</td>\n",
       "      <td>-0.032356</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.002864</td>\n",
       "      <td>-0.025930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.089063</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.011595</td>\n",
       "      <td>-0.036656</td>\n",
       "      <td>0.012191</td>\n",
       "      <td>0.024991</td>\n",
       "      <td>-0.036038</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.022692</td>\n",
       "      <td>-0.009362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005383</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.036385</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>0.003935</td>\n",
       "      <td>0.015596</td>\n",
       "      <td>0.008142</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>-0.031991</td>\n",
       "      <td>-0.046641</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age       sex       bmi       map        tc       ldl       hdl  \\\n",
       "0  0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401   \n",
       "1 -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412   \n",
       "2  0.085299  0.050680  0.044451 -0.005671 -0.045599 -0.034194 -0.032356   \n",
       "3 -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038   \n",
       "4  0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142   \n",
       "\n",
       "        tch       ltg       glu  \n",
       "0 -0.002592  0.019908 -0.017646  \n",
       "1 -0.039493 -0.068330 -0.092204  \n",
       "2 -0.002592  0.002864 -0.025930  \n",
       "3  0.034309  0.022692 -0.009362  \n",
       "4 -0.002592 -0.031991 -0.046641  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And our target array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 75., 141., 206., 135.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the `train_test_split` function in order to make the split. The `test_size=0.2` inside the function indicates the percentage of the data that should be held over for testing. It’s usually around 80/20 or 70/30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:  (353, 10) (353,)\n",
      "Testing set:  (89, 10) (89,)\n"
     ]
    }
   ],
   "source": [
    "# create training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2)\n",
    "print(\"Training set: \", X_train.shape, y_train.shape)\n",
    "print(\"Testing set: \", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Setting up validation and test sets\n",
    "\n",
    "## 5.1. The testing set should reflect real data application\n",
    "\n",
    "Say you’re building a startup that will provide an endless stream of cat pictures to cat lovers. You use a neural network to build a computer vision system for detecting cats in pictures. You run a mobile app, and users are\n",
    "uploading pictures of many different things to your app. You want to automatically find the cat pictures.\n",
    "\n",
    "Your team gets a large training set by downloading pictures of cats (positive examples) and non-cats (negative examples) off of different websites. They split the dataset 70%/30% into training and test sets. Using this data, they build a cat detector that works well on the training and test sets. But when you deploy this classifier into the mobile app, you find that the performance is really poor!\n",
    "\n",
    "What happened? You figure out that the pictures users are uploading have a different look than the website\n",
    "images that make up your training set: Users are uploading pictures taken with mobile phones, which tend to be lower resolution, blurrier, and poorly lit. Since your training/test sets were made of website images, your algorithm did not generalize well to the actual distribution you care about: mobile phone pictures.\n",
    "\n",
    "Before the modern era of big data, it was a common rule in machine learning to use a random 70%/30% split to form your training and test sets. This practice can work, but it’s a bad idea in more and more applications where the training distribution (website images in our example above) is different from the distribution you ultimately care about (mobile phone images).\n",
    "\n",
    "Once you define a val set (developement set) and test set, your team will try a lot of ideas, such as different learning algorithm parameters, to see what works best. The val and test sets allow your team to quickly see how well your algorithm is doing. In other words, the purpose of the val and test sets are to direct your team toward\n",
    "the most important changes to make to the machine learning system.\n",
    "\n",
    "So, you should do the following:\n",
    "> **Choose val and test sets to reflect data you expect to get in the future and want to do well on.**\n",
    "\n",
    "In other words, your test set should not simply be 30% of the available data, especially if you expect your future data (mobile phone images) to be different in nature from your training set (website images).\n",
    "\n",
    "If you have not yet launched your mobile app, you might not have any users yet, and thus might not be able to get data that accurately reflects what you have to do well on in the future. But you might still try to approximate this. For example, ask your friends to take mobile phone pictures of cats and send them to you. Once your app is launched, you can update your val/test sets using actual user data.\n",
    "\n",
    "If you really don’t have any way of getting data that approximates what you expect to get in the future, perhaps you can start by using website images. But you should be aware of the risk of this leading to a system that doesn’t generalize well. It requires judgment to decide how much to invest in valeloping great val and test sets. But\n",
    "don’t assume your training distribution is the same as your test distribution. Try to pick test examples that reflect what you ultimately want to perform well on, rather than whatever data you happen to have for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Your val and test sets should come from the same distribution\n",
    "\n",
    "You have your cat app image data segmented into four regions, based on your largest\n",
    "markets: (i) US, (ii) China, (iii) India, and (iv) Other. To come up with a val set and a test\n",
    "set, say we put US and India in the val set; China and Other in the test set. In other words,\n",
    "we can randomly assign two of these segments to the val set, and the other two to the test\n",
    "set, right?\n",
    "Once you define the val and test sets, your team will be focused on improving val set\n",
    "performance. Thus, the val set should reflect the task you want to improve on the most: To\n",
    "do well on all four geographies, and not only two.\n",
    "There is a second problem with having different val and test set distributions: There is a\n",
    "chance that your team will build something that works well on the val set, only to find that it\n",
    "does poorly on the test set. I’ve seen this result in much frustration and wasted effort. Avoid\n",
    "letting this happen to you.\n",
    "As an example, suppose your team valelops a system that works well on the val set but not\n",
    "the test set. If your val and test sets had come from the same distribution, then you would\n",
    "have a very clear diagnosis of what went wrong: You have overfit the val set. The obvious\n",
    "cure is to get more val set data.\n",
    "But if the val and test sets come from different distributions, then your options are less\n",
    "clear. Several things could have gone wrong:\n",
    "\n",
    "* You had overfit to the val set.\n",
    "* The test set is harder than the val set. So your algorithm might be doing as well as could be expected, and no further significant improvement is possible.\n",
    "* The test set is not necessarily harder, but just different, from the val set. So what works well on the val set just does not work well on the test set. In this case, a lot of your work to improve val set performance might be wasted effort.\n",
    "\n",
    "Working on machine learning applications is hard enough. Having mismatched val and test\n",
    "sets introduces additional uncertainty about whether improving on the val set distribution\n",
    "also improves test set performance. Having mismatched val and test sets makes it harder to\n",
    "figure out what is and isn’t working, and thus makes it harder to prioritize what to work on.\n",
    "If you are working on a 3rd party benchmark problem, their creator might have specified val\n",
    "and test sets that come from different distributions. Luck, rather than skill, will have a\n",
    "greater impact on your performance on such benchmarks compared to if the val and test\n",
    "sets come from the same distribution. It is an important research problem to valelop\n",
    "learning algorithms that are trained on one distribution and generalize well to another. But if\n",
    "your goal is to make progress on a specific machine learning application rather than make\n",
    "research progress, I recommend trying to choose val and test sets that are drawn from the\n",
    "same distribution. This will make your team more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. How large do the val/test sets need to be?\n",
    "\n",
    "The val set should be large enough to detect differences between algorithms that you are\n",
    "trying out. For example, if classifier A has an accuracy of 90.0% and classifier B has an\n",
    "accuracy of 90.1%, then a val set of 100 examples would not be able to detect this 0.1%\n",
    "difference. Compared to other machine learning problems I’ve seen, a 100 example val set is\n",
    "small. val sets with sizes from 1,000 to 10,000 examples are common. With 10,000\n",
    "examples, you will have a good chance of detecting an improvement of 0.1%.2\n",
    "For mature and important applications—for example, advertising, web search, and product\n",
    "recommendations—I have also seen teams that are highly motivated to eke out even a 0.01%\n",
    "improvement, since it has a direct impact on the company’s profits. In this case, the val set\n",
    "could be much larger than 10,000, in order to detect even smaller improvements.\n",
    "How about the size of the test set? It should be large enough to give high confidence in the\n",
    "overall performance of your system. One popular heuristic had been to use 30% of your data\n",
    "for your test set. This works well when you have a modest number of examples—say 100 to\n",
    "10,000 examples. But in the era of big data where we now have machine learning problems\n",
    "with sometimes more than a billion examples, the fraction of data allocated to val/test sets\n",
    "has been shrinking, even as the absolute number of examples in the val/test sets has been\n",
    "growing. There is no need to have excessively large val/test sets beyond what is needed to\n",
    "evaluate the performance of your algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Splitting Data with tf.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(10, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "full_dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) \n",
    "full_dataset = full_dataset.shuffle(10)\n",
    "\n",
    "num_elements = 0\n",
    "for element in full_dataset:\n",
    "    print(element)\n",
    "    num_elements += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "DATASET_SIZE = num_elements\n",
    "print(DATASET_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.7 * DATASET_SIZE)\n",
    "val_size = int(0.15 * DATASET_SIZE)\n",
    "test_size = int(0.15 * DATASET_SIZE)\n",
    "\n",
    "train_dataset = full_dataset.take(train_size)\n",
    "val_dataset = full_dataset.skip(train_size)\n",
    "test_dataset = val_dataset.skip(val_size)\n",
    "val_dataset = val_dataset.take(val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(10, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "num_elements = 0\n",
    "for element in train_dataset:\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(2, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for element in val_dataset:\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for element in test_dataset:\n",
    "    print(element)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlcourse-venv",
   "language": "python",
   "name": "dlcourse-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
